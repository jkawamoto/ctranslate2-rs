[package]
name = "ct2rs"
version = "0.9.7"
authors = ["Junpei Kawamoto <kawamoto.junpei@gmail.com>"]
categories = ["api-bindings"]
documentation = "https://docs.rs/ct2rs"
edition = "2021"
include = [
    "/include",
    "/src",
    "/build.rs",
    "/LICENSE",
    "/examples",
    "/CTranslate2/CMakeLists.txt",
    "/CTranslate2/LICENSE",
    "/CTranslate2/cmake",
    "/CTranslate2/cpp",
    "/CTranslate2/include",
    "/CTranslate2/python",
    "/CTranslate2/src",
    "/CTranslate2/third_party",
    "!/CTranslate2/**/.*",
    "!/CTranslate2/**/test",
    "!/CTranslate2/**/docs",
    "!/CTranslate2/**/media",
]
keywords = ["ctranslate2", "bindings", "llm", "whisper"]
license = "MIT"
repository = "https://github.com/jkawamoto/ctranslate2-rs"
description = "Rust bindings for OpenNMT/CTranslate2"

[dependencies]
anyhow = "1.0.98"
cxx = { version = "1.0", features = ["c++17"] }
sentencepiece = { version = "0.11.3", optional = true }
tokenizers = { version = "0.21", optional = true }

# Dependencies for Whisper model
mel_spec = { version = "0.3", optional = true }
ndarray = { version = "0.16.1", optional = true }
serde = { version = "1.0.219", features = ["derive"], optional = true }
serde_json = { version = "1.0", optional = true }

# Dependencies for Hugging Face integration
hf-hub = { version = "0.4", optional = true }

[target.'cfg(windows)'.dependencies]
intel-mkl-src = { version = "0.8.1", optional = true, features = ["mkl-static-ilp64-seq"] }

[target.'cfg(unix)'.dependencies]
intel-mkl-src = { version = "0.8.1", optional = true }

[dev-dependencies]
clap = { version = "4.5", features = ["derive"] }
rand = "0.9.1"

# Dependencies for Whisper example
hound = { version = "3.5.1" }

[build-dependencies]
cmake = "0.1.54"
cxx-build = "1.0"
walkdir = "2.5.0"
ureq = "3.0.12"
flate2 = "1.1.2"
tar = "0.4.44"

[features]
default = ["all-tokenizers", "os-defaults", "cuda-small-binary"]
os-defaults = []
whisper = [
    "dep:mel_spec",
    "dep:ndarray",
    "dep:serde",
    "dep:serde_json",
    "all-tokenizers",
]
flash-attention = []
tensor-parallel = []
cuda-dynamic-loading = ["cuda"]
hub = ["dep:hf-hub"]
all-tokenizers = ["sentencepiece", "tokenizers"]
sentencepiece = ["dep:sentencepiece"]
tokenizers = ["dep:tokenizers"]

# Features to select backends.
mkl = ["dep:intel-mkl-src"]
openblas = []
dnnl = []
openmp-runtime-comp = []
openmp-runtime-intel = []
ruy = []
accelerate = []
cuda = []
cuda-small-binary = []
msse4_1 = []
cudnn = ["cuda"]
vendored = []
_vendor = []

[[example]]
name = "bart"

[[example]]
name = "bloom"

[[example]]
name = "falcon"

[[example]]
name = "marian-mt"

[[example]]
name = "mpt"

[[example]]
name = "nllb"

[[example]]
name = "gpt-2"

[[example]]
name = "gpt-j"

[[example]]
name = "gpt-neox"

[[example]]
name = "opt"
required-features = ["tokenizers"]

[[example]]
name = "t5"

[[example]]
name = "stream"

[[example]]
name = "whisper"
required-features = ["whisper"]

[package.metadata.docs.rs]
features = ["whisper", "hub"]
